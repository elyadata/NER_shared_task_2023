{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/mohammedkhalilia/72c3261734d7715094089bdf4de74b4a/train_flat_arabic_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4EvIlcrssU6"
   },
   "outputs": [],
   "source": [
    "# Add the ArabicNER package to the system path\n",
    "import sys\n",
    "import argparse\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "# Import train function\n",
    "from arabiner.bin.train import main as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqqvY1fXtZpD"
   },
   "outputs": [],
   "source": [
    "# Setup the model arguments\n",
    "args_dict = {\n",
    "    # Model output path to save artifacts and model predictions\n",
    "    \"output_path\": \"../output/\",\n",
    "\n",
    "    # train/test/validation data paths\n",
    "    \"train_path\": \"../data/train.txt\",\n",
    "    \"test_path\": \"../data/test.txt\",\n",
    "    \"val_path\": \"../data/val.txt\",\n",
    "\n",
    "    # seed for randomization\n",
    "    \"seed\": 1,\n",
    "\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    # Nmber of workers for the dataloader\n",
    "    \"num_workers\": 0,\n",
    "\n",
    "    # GPU/device Ids to train model on\n",
    "    # For two GPUs use [0, 1]\n",
    "    # For three GPUs use [0, 1, 2], etc.\n",
    "    \"gpus\": [0,1],\n",
    "\n",
    "    # Overwrite data in output_path directory specified above\n",
    "    \"overwrite\": True,\n",
    "\n",
    "    # How often to print the logs in terms of number of steps\n",
    "    \"log_interval\": 10,\n",
    "\n",
    "    # Data configuration\n",
    "    # Here we specify the dataset class and there are two options:\n",
    "    #  arabiner.data.datasets.DefaultDataset: for flat NER \n",
    "    #  arabiner.data.datasets.NestedTagsDataset: for nested NER\n",
    "    #\n",
    "    # kwargs: keyword arguments to the dataset class\n",
    "    # This notebook used the DefaultDataset for flat NER\n",
    "    \"data_config\": {\n",
    "        \"fn\": \"arabiner.data.datasets.DefaultDataset\", \n",
    "        \"kwargs\": {\"max_seq_len\": 512}\n",
    "    },\n",
    "\n",
    "    # Neural net configuration\n",
    "    # There are two NNs:\n",
    "    #   arabiner.nn.BertSeqTagger: flat NER tagger\n",
    "    #   arabiner.nn.BertNestedTagger: nested NER tagger\n",
    "    #\n",
    "    # kwargs: keyword arguments to the NN\n",
    "    # This notebook uses BertSeqTagger for flat NER tagging\n",
    "    \"network_config\": {\n",
    "        \"fn\": \"arabiner.nn.BertSeqTagger\", \n",
    "        \"kwargs\": {\"dropout\": 0.1, \"bert_model\": \"aubmindlab/bert-base-arabertv2\"}\n",
    "    },\n",
    "\n",
    "    # Model trainer configuration\n",
    "    #\n",
    "    #  arabiner.trainers.BertTrainer: for flat NER training\n",
    "    #  arabiner.trainers.BertNestedTrainer: for nested NER training\n",
    "    #\n",
    "    # kwargs: keyword arguments to arabiner.trainers.BertTrainer\n",
    "    #         additional arguments you can pass includes\n",
    "    #           - clip: for gradient clipping\n",
    "    #           - patience: number of epochs for early termination\n",
    "    # This notebook uses BertTrainer for fat NER training\n",
    "    \"trainer_config\": {\n",
    "        \"fn\": \"arabiner.trainers.BertTrainer\", \n",
    "        \"kwargs\": {\"max_epochs\": 50}\n",
    "    },\n",
    "\n",
    "    # Optimizer configuration\n",
    "    # Our experiments use torch.optim.AdamW, however, you are free to pass\n",
    "    # any other optmizers such as torch.optim.Adam or torch.optim.SGD\n",
    "    # lr: learning rate\n",
    "    # kwargs: keyword arguments to torch.optim.AdamW or whatever optimizer you use\n",
    "    #\n",
    "    # Additional optimizers can be found here:\n",
    "    # https://pytorch.org/docs/stable/optim.html\n",
    "    \"optimizer\": {\n",
    "        \"fn\": \"torch.optim.AdamW\", \n",
    "        \"kwargs\": {\"lr\": 0.0001}\n",
    "    },\n",
    "\n",
    "    # Learning rate scheduler configuration\n",
    "    # You can pass a learning scheduler such as torch.optim.lr_scheduler.StepLR\n",
    "    # kwargs: keyword arguments to torch.optim.AdamW or whatever scheduler you use\n",
    "    #\n",
    "    # Additional schedulers can be found here:\n",
    "    # https://pytorch.org/docs/stable/optim.html\n",
    "    \"lr_scheduler\": {\n",
    "        \"fn\": \"torch.optim.lr_scheduler.ExponentialLR\", \n",
    "        \"kwargs\": {\"gamma\": 1}\n",
    "    },\n",
    "\n",
    "    # Loss function configuration\n",
    "    # We use cross entropy loss\n",
    "    # kwargs: keyword arguments to torch.nn.CrossEntropyLoss or whatever loss function you use\n",
    "    \"loss\": {\n",
    "        \"fn\": \"torch.nn.CrossEntropyLoss\", \n",
    "        \"kwargs\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert args dictionary to argparse namespace\n",
    "args = argparse.Namespace()\n",
    "args.__dict__ = args_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJQMuAC8taZB"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMWji+1pnVsUrQ1qxgnF4kv",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
