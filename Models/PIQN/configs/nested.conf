train_path = data/datasets/wojood/train.json
valid_path = data/datasets/wojood/val.json
save_path = data/checkpoints/wojood
save_path_include_iteration = False
init_eval = False
save_optimizer = True
train_log_iter = 1
final_eval = False
train_batch_size = 4
epochs = 100
lr = 2e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 30
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/wojood/types.json
# tokenizer_path = dmis-lab/biobert-large-cased-v1.1
tokenizer_path = PIQN/data/checkpoints/wojood/wojood_train/2023-07-05_18:18:00.529107/best_model
lowercase = False
sampling_processes = 4
label = wojood_train
log_path = data/checkpoints/wojood
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 3
# model_path = dmis-lab/biobert-large-cased-v1.1
model_path = PIQN/data/checkpoints/wojood/wojood_train/2023-07-05_18:18:00.529107/best_model
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = False
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../biovec/PubMed-shuffle-win-30.txt
share_query_pos = True
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
use_entity_attention = True
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = False
mask_entself = False
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = False
word_mask_entself = False
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = True
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = PIQN/data/cache/wojood_biobert

# train_path = data/datasets/ace05/ace05_train_context.json
# valid_path = data/datasets/ace05/ace05_dev_context.json
# save_path = data/checkpoints/ace05
# save_path_include_iteration = False
# init_eval = False
# save_optimizer = False
# train_log_iter = 1
# final_eval = False
# train_batch_size = 8
# epochs = 50
# lr = 2e-05
# lr_warmup = 0.1
# weight_decay = 0.01
# max_grad_norm = 1.0
# match_solver = hungarian
# type_loss = celoss
# match_warmup_epoch = 0
# nil_weight = -1.0
# match_boundary_weight = 2.0
# match_class_weight = 2.0
# loss_boundary_weight = 2.0
# loss_class_weight = 2.0
# use_aux_loss = True
# deeply_weight = same
# use_masked_lm = False
# repeat_gt_entities = 45
# split_epoch = 5
# copy_weight = False
# local_rank = -1
# world_size = -1
# types_path = data/datasets/ace05/ace05_types.json
# tokenizer_path = bert-large-cased
# lowercase = False
# sampling_processes = 4
# label = ace05_train
# log_path = data/checkpoints/ace05
# store_predictions = True
# store_examples = True
# example_count = None
# debug = False
# device_id = 0
# model_path = bert-large-cased
# model_type = piqn
# cpu = False
# eval_batch_size = 8
# prop_drop = 0.5
# freeze_transformer = False
# no_overlapping = False
# no_partial_overlapping = True
# no_duplicate = True
# cls_threshold = 0.0
# boundary_threshold = 0.0
# pos_size = 25
# char_lstm_layers = 1
# lstm_layers = 3
# char_size = 50
# char_lstm_drop = 0.2
# use_glove = False
# use_pos = False
# use_char_lstm = False
# use_lstm = True
# pool_type = max
# wordvec_path = ../glove/glove.6B.100d.txt
# share_query_pos = True
# use_token_level_encoder = True
# num_token_entity_encoderlayer = 6
# use_entity_attention = False
# entity_queries_num = 60
# entity_emb_size = None
# mask_ent2tok = True
# mask_tok2ent = False
# mask_ent2ent = False
# mask_entself = False
# word_mask_ent2tok = True
# word_mask_tok2ent = False
# word_mask_ent2ent = False
# word_mask_entself = True
# entity_aware_attention = False
# entity_aware_selfout = False
# entity_aware_intermediate = False
# entity_aware_output = False
# use_entity_pos = True
# use_entity_common_embedding = True
# inlcude_subword_aux_loss = False
# last_layer_for_loss = 3
# seed = 47
# cache_path = None

# train_path = data/datasets/ace04/ace04_train_context.json
# valid_path = data/datasets/ace04/ace04_dev_context.json
# save_path = data/checkpoints/ace04
# save_path_include_iteration = False
# init_eval = False
# save_optimizer = False
# train_log_iter = 1
# final_eval = False
# train_batch_size = 8
# epochs = 50
# lr = 2e-05
# lr_warmup = 0.1
# weight_decay = 0.01
# max_grad_norm = 1.0
# match_solver = hungarian
# type_loss = celoss
# match_warmup_epoch = 0
# nil_weight = -1.0
# match_boundary_weight = 2.0
# match_class_weight = 2.0
# loss_boundary_weight = 2.0
# loss_class_weight = 2.0
# use_aux_loss = True
# deeply_weight = same
# use_masked_lm = False
# repeat_gt_entities = 45
# split_epoch = 5
# copy_weight = False
# local_rank = -1
# world_size = -1
# types_path = data/datasets/ace04/ace04_types.json
# tokenizer_path = bert-large-cased
# lowercase = False
# sampling_processes = 4
# label = ace04_train
# log_path = data/checkpoints/ace04
# store_predictions = True
# store_examples = True
# example_count = None
# debug = False
# device_id = 2
# model_path = bert-large-cased
# model_type = piqn
# cpu = False
# eval_batch_size = 8
# prop_drop = 0.5
# freeze_transformer = False
# no_overlapping = False
# no_partial_overlapping = True
# no_duplicate = True
# cls_threshold = 0.0
# boundary_threshold = 0.0
# pos_size = 25
# char_lstm_layers = 1
# lstm_layers = 3
# char_size = 50
# char_lstm_drop = 0.2
# use_glove = False
# use_pos = False
# use_char_lstm = False
# use_lstm = True
# pool_type = max
# wordvec_path = ../glove/glove.6B.100d.txt
# share_query_pos = False
# use_token_level_encoder = True
# num_token_entity_encoderlayer = 5
# use_entity_attention = True
# entity_queries_num = 60
# entity_emb_size = None
# mask_ent2tok = True
# mask_tok2ent = False
# mask_ent2ent = False
# mask_entself = False
# word_mask_ent2tok = True
# word_mask_tok2ent = False
# word_mask_ent2ent = False
# word_mask_entself = False
# entity_aware_attention = False
# entity_aware_selfout = False
# entity_aware_intermediate = False
# entity_aware_output = False
# use_entity_pos = True
# use_entity_common_embedding = True
# inlcude_subword_aux_loss = False
# last_layer_for_loss = 3
# seed = 47
# cache_path = None

# train_path = data/datasets/genia/genia_train_dev_context.json
# valid_path = data/datasets/genia/genia_test_context.json
# save_path = data/checkpoints/genia
# save_path_include_iteration = False
# init_eval = False
# save_optimizer = False
# train_log_iter = 1
# final_eval = False
# train_batch_size = 4
# epochs = 30
# lr = 2e-05
# lr_warmup = 0.05
# weight_decay = 0.01
# max_grad_norm = 1.0
# match_solver = hungarian
# type_loss = celoss
# match_warmup_epoch = 0
# nil_weight = -1.0
# match_boundary_weight = 2.0
# match_class_weight = 2.0
# loss_boundary_weight = 2.0
# loss_class_weight = 2.0
# use_aux_loss = True
# deeply_weight = same
# use_masked_lm = False
# repeat_gt_entities = 30
# split_epoch = 5
# copy_weight = False
# local_rank = -1
# world_size = -1
# types_path = data/datasets/genia/genia_types.json
# tokenizer_path = dmis-lab/biobert-large-cased-v1.1
# lowercase = False
# sampling_processes = 4
# label = genia_train
# log_path = data/checkpoints/genia
# store_predictions = True
# store_examples = True
# example_count = None
# debug = False
# device_id = 3
# model_path = dmis-lab/biobert-large-cased-v1.1
# model_type = piqn
# cpu = False
# eval_batch_size = 8
# prop_drop = 0.5
# freeze_transformer = False
# no_overlapping = False
# no_partial_overlapping = True
# no_duplicate = True
# cls_threshold = 0.0
# boundary_threshold = 0.0
# pos_size = 25
# char_lstm_layers = 1
# lstm_layers = 3
# char_size = 50
# char_lstm_drop = 0.2
# use_glove = False
# use_pos = False
# use_char_lstm = False
# use_lstm = True
# pool_type = max
# wordvec_path = ../biovec/PubMed-shuffle-win-30.txt
# share_query_pos = True
# use_token_level_encoder = True
# num_token_entity_encoderlayer = 5
# use_entity_attention = True
# entity_queries_num = 60
# entity_emb_size = None
# mask_ent2tok = True
# mask_tok2ent = False
# mask_ent2ent = False
# mask_entself = False
# word_mask_ent2tok = True
# word_mask_tok2ent = False
# word_mask_ent2ent = False
# word_mask_entself = False
# entity_aware_attention = False
# entity_aware_selfout = False
# entity_aware_intermediate = False
# entity_aware_output = False
# use_entity_pos = True
# use_entity_common_embedding = True
# inlcude_subword_aux_loss = False
# last_layer_for_loss = 3
# seed = 47
# cache_path = None

# train_path = data/datasets/kbp17/kbp17_train_context.json
# valid_path = data/datasets/kbp17/kbp17_dev_context.json
# save_path = data/checkpoints/kbp17
# save_path_include_iteration = False
# init_eval = False
# save_optimizer = False
# train_log_iter = 1
# final_eval = False
# train_batch_size = 4
# epochs = 30
# lr = 3e-05
# lr_warmup = 0.05
# weight_decay = 0.01
# max_grad_norm = 1.0
# match_solver = hungarian
# type_loss = celoss
# match_warmup_epoch = 0
# nil_weight = -1.0
# match_boundary_weight = 1.0
# match_class_weight = 2.0
# loss_boundary_weight = 2.0
# loss_class_weight = 2.0
# use_aux_loss = True
# deeply_weight = same
# use_masked_lm = False
# repeat_gt_entities = 30
# split_epoch = 5
# copy_weight = True
# local_rank = -1
# world_size = -1
# types_path = data/datasets/kbp17/kbp17_types.json
# tokenizer_path = bert-base-cased
# lowercase = False
# sampling_processes = 4
# label = kbp17_train
# log_path = data/checkpoints/kbp17
# store_predictions = True
# store_examples = True
# example_count = None
# debug = False
# device_id = 3
# model_path = bert-base-cased
# model_type = piqn
# cpu = False
# eval_batch_size = 8
# prop_drop = 0.5
# freeze_transformer = False
# no_overlapping = False
# no_partial_overlapping = True
# no_duplicate = True
# cls_threshold = 0.0
# boundary_threshold = 0.0
# pos_size = 25
# char_lstm_layers = 1
# lstm_layers = 3
# char_size = 50
# char_lstm_drop = 0.2
# use_glove = False
# use_pos = False
# use_char_lstm = False
# use_lstm = True
# pool_type = max
# wordvec_path = ../glove/glove.6B.100d.txt
# share_query_pos = False
# use_token_level_encoder = True
# num_token_entity_encoderlayer = 3
# use_entity_attention = True
# entity_queries_num = 60
# entity_emb_size = None
# mask_ent2tok = True
# mask_tok2ent = False
# mask_ent2ent = False
# mask_entself = False
# word_mask_ent2tok = True
# word_mask_tok2ent = False
# word_mask_ent2ent = False
# word_mask_entself = False
# entity_aware_attention = False
# entity_aware_selfout = False
# entity_aware_intermediate = False
# entity_aware_output = False
# use_entity_pos = True
# use_entity_common_embedding = True
# inlcude_subword_aux_loss = False
# last_layer_for_loss = 3
# seed = 47
# cache_path = None

# train_path = data/datasets/nne/nne_train_context.json
# valid_path = data/datasets/nne/nne_dev_context.json
# save_path = data/checkpoints/nne
# save_path_include_iteration = False
# init_eval = False
# save_optimizer = False
# train_log_iter = 1
# final_eval = False
# train_batch_size = 4
# epochs = 50
# lr = 2e-05
# lr_warmup = 0.05
# weight_decay = 0.01
# max_grad_norm = 1.0
# match_solver = hungarian
# type_loss = celoss
# match_warmup_epoch = 0
# nil_weight = -1.0
# match_boundary_weight = 2.0
# match_class_weight = 2.0
# loss_boundary_weight = 2.0
# loss_class_weight = 2.0
# use_aux_loss = True
# deeply_weight = same
# use_masked_lm = False
# repeat_gt_entities = 30
# split_epoch = 5
# copy_weight = False
# local_rank = -1
# world_size = -1
# types_path = data/datasets/nne/nne_types.json
# tokenizer_path = bert-large-cased
# lowercase = False
# sampling_processes = 4
# label = nne_train
# log_path = data/checkpoints/nne
# store_predictions = True
# store_examples = True
# example_count = None
# debug = False
# device_id = 3
# model_path = bert-large-cased
# model_type = piqn
# cpu = False
# eval_batch_size = 8
# prop_drop = 0.5
# freeze_transformer = False
# no_overlapping = False
# no_partial_overlapping = True
# no_duplicate = True
# cls_threshold = 0.0
# boundary_threshold = 0.0
# pos_size = 25
# char_lstm_layers = 1
# lstm_layers = 3
# char_size = 50
# char_lstm_drop = 0.2
# use_glove = False
# use_pos = False
# use_char_lstm = False
# use_lstm = True
# pool_type = max
# wordvec_path = ../glove/glove.6B.100d.txt
# share_query_pos = True
# use_token_level_encoder = True
# num_token_entity_encoderlayer = 5
# use_entity_attention = False
# entity_queries_num = 60
# entity_emb_size = None
# mask_ent2tok = True
# mask_tok2ent = False
# mask_ent2ent = False
# mask_entself = False
# word_mask_ent2tok = True
# word_mask_tok2ent = False
# word_mask_ent2ent = False
# word_mask_entself = False
# entity_aware_attention = False
# entity_aware_selfout = False
# entity_aware_intermediate = False
# entity_aware_output = False
# use_entity_pos = True
# use_entity_common_embedding = True
# inlcude_subword_aux_loss = False
# last_layer_for_loss = 3
# seed = 47
# cache_path = None
