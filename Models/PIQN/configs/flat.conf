train_path = data/datasets/zhmsra/zhmsra_train_context.json
valid_path = data/datasets/zhmsra/zhmsra_dev_context.json
save_path = data/checkpoints/zhmsra
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 8
epochs = 45
lr = 5e-05
lr_warmup = 0.1
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 1.0
match_class_weight = 1.0
loss_boundary_weight = 1.0
loss_class_weight = 1.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 45
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/zhmsra/zhmsra_types.json
tokenizer_path = hfl/chinese-bert-wwm-ext
lowercase = False
sampling_processes = 4
label = zhmsra_train
log_path = data/checkpoints/zhmsra
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 2
model_path = hfl/chinese-bert-wwm-ext
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = True
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 2
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = True
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
use_entity_attention = True
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = True
mask_entself = True
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = True
word_mask_entself = True
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = True
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None

train_path = data/datasets/conll03/conll03_train_dev_context.json
valid_path = data/datasets/conll03/conll03_test_context.json
save_path = data/checkpoints/conll03
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 8
epochs = 30
lr = 2e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 45
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/conll03/conll03_types.json
tokenizer_path = bert-large-cased
lowercase = False
sampling_processes = 4
label = conll03_train
log_path = data/checkpoints/conll03
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 2
model_path = bert-large-cased
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = True
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = False
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
use_entity_attention = False
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = True
mask_entself = True
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = True
word_mask_entself = True
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = False
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None

train_path = data/datasets/ontonotes/ontonotes_train_context.json
valid_path = data/datasets/ontonotes/ontonotes_dev_context.json
save_path = data/checkpoints/ontonotes
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 4
epochs = 30
lr = 2e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 5.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 45
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/ontonotes/ontonotes_types.json
tokenizer_path = bert-large-cased
lowercase = False
sampling_processes = 4
label = ontonotes_train
log_path = data/checkpoints/ontonotes
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 1
model_path = bert-large-cased
model_type = piqn
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = True
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = False
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
use_entity_attention = False
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = True
mask_entself = True
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = True
word_mask_entself = True
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = False
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None

train_path = data/datasets/fewnerd/fewnerd_train_context.json
valid_path = data/datasets/fewnerd/fewnerd_dev_context.json
save_path = data/checkpoints/fewnerd
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 16
epochs = 30
lr = 3e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 5.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
use_masked_lm = False
repeat_gt_entities = 30
split_epoch = 5
copy_weight = False
local_rank = -1
world_size = -1
types_path = data/datasets/fewnerd/fewnerd_types.json
tokenizer_path = bert-base-cased
lowercase = False
sampling_processes = 4
label = fewnerd_train
log_path = data/checkpoints/fewnerd
store_predictions = True
store_examples = True
example_count = None
debug = False
device_id = 2
model_path = bert-base-cased
model_type = piqn
cpu = False
eval_batch_size = 16
prop_drop = 0.5
freeze_transformer = False
no_overlapping = True
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 1
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
use_lstm = True
pool_type = max
wordvec_path = ../glove/glove.6B.100d.txt
share_query_pos = True
use_token_level_encoder = True
num_token_entity_encoderlayer = 3
use_entity_attention = False
entity_queries_num = 60
entity_emb_size = None
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = False
mask_entself = False
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = False
word_mask_entself = False
entity_aware_attention = False
entity_aware_selfout = False
entity_aware_intermediate = False
entity_aware_output = False
use_entity_pos = True
use_entity_common_embedding = False
inlcude_subword_aux_loss = False
last_layer_for_loss = 3
seed = 47
cache_path = None
